{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "alive-toner",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-20T07:25:57.277979Z",
     "iopub.status.busy": "2021-06-20T07:25:57.277207Z",
     "iopub.status.idle": "2021-06-20T07:25:59.265102Z",
     "shell.execute_reply": "2021-06-20T07:25:59.264376Z",
     "shell.execute_reply.started": "2021-06-20T07:19:37.088866Z"
    },
    "id": "dPAV-0nFRQE7",
    "papermill": {
     "duration": 2.011003,
     "end_time": "2021-06-20T07:25:59.265287",
     "exception": false,
     "start_time": "2021-06-20T07:25:57.254284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing the librtaries\n",
    "import numpy as np\n",
    "import nltk\n",
    "import underthesea\n",
    "import re\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import heapq\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "import gzip\n",
    "import shutil\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "brown-robin",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-20T07:25:59.290346Z",
     "iopub.status.busy": "2021-06-20T07:25:59.288394Z",
     "iopub.status.idle": "2021-06-20T07:25:59.291128Z",
     "shell.execute_reply": "2021-06-20T07:25:59.291588Z",
     "shell.execute_reply.started": "2021-06-20T07:19:39.165116Z"
    },
    "id": "UUfONi2IRTe0",
    "papermill": {
     "duration": 0.017445,
     "end_time": "2021-06-20T07:25:59.291771",
     "exception": false,
     "start_time": "2021-06-20T07:25:59.274326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text from wikipedia about Elon Musk\n",
    "txt = \"\"\"1. Hướng dẫn sử dụng nhà vệ sinh:\n",
    "\n",
    "- Tư thế ngồi: ngồi hẳn xuống bệ ngồi. Không được đứng trên thành bồn hoặc cho chân lên ngồm xổm.\n",
    "\n",
    "- Sau khi đi vệ sinh xong, bạn cần xả nước cho sạch. Nếu đi đại tiện, bạn có thể đậy nắp rồi mới nhấn nút xả, tránh nước bẩn văng vào người.\n",
    "\n",
    "- Các loại giấy rác cần vứt vào thùng rác.\n",
    "\n",
    "- Ngoài ra, hãy để vòi xịt vào đúng vị trí ban đầu.\n",
    "\n",
    "2. Hướng dẫn sử dụng bồn rửa tay:\n",
    "\n",
    "- Bước 1: Chà xát hai hai bàn tay với xà phòng (hoặc nước rửa tay) để sát khuẩn\n",
    "\n",
    "- Bước 2: Bật nước và rửa tay dưới dòng nước sạch.\n",
    "\n",
    "- Bước 3: Tắt vòi nước và kiểm tra kĩ, tránh trường hợp nước chảy nhỏ giọt.\n",
    "\n",
    "- Lưu ý:\n",
    "\n",
    "+ Bạn không được vứt rác, tóc, giấy vào bồn rửa tay, tránh trường hợp tắc ống.\n",
    "\n",
    "+ Không được tác động mạnh vào bồn.\n",
    "\n",
    "3. Hướng dẫn sử dụng máy sấy tay:\n",
    "\n",
    "- Bước 1: Đưa tay vào dưới máy sấy, úp tay, duỗi thẳng các ngón tay. Khi có gió (khí) thổi xuống, bạn đưa tay của mình ra vào từ từ, nhiều lần.\n",
    "\n",
    "- Bước 2: Ngửa lòng bàn tay lên và giữa nguyên thao tác như bước 1.\n",
    "\n",
    "- Bước 3: Xoa hai tay với nhau để tay khô hẳn.\n",
    "\n",
    "Lưu ý: máy sấy sẽ tự động tắt khi hoạt động liên tục trong 60 giây. Nếu muốn tiếp tục sử dụng, bạn hãy đưa tay ra khỏi thiết bị và làm lại lần lượt 3 bước trên.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "78967c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.',\n",
       " 'Hướng dẫn sử dụng nhà vệ sinh:\\n\\n- Tư thế ngồi: ngồi hẳn xuống bệ ngồi.',\n",
       " 'Không được đứng trên thành bồn hoặc cho chân lên ngồm xổm.',\n",
       " '- Sau khi đi vệ sinh xong, bạn cần xả nước cho sạch.',\n",
       " 'Nếu đi đại tiện, bạn có thể đậy nắp rồi mới nhấn nút xả, tránh nước bẩn văng vào người.',\n",
       " '- Các loại giấy rác cần vứt vào thùng rác.',\n",
       " '- Ngoài ra, hãy để vòi xịt vào đúng vị trí ban đầu.',\n",
       " '2.',\n",
       " 'Hướng dẫn sử dụng bồn rửa tay:\\n\\n- Bước 1: Chà xát hai hai bàn tay với xà phòng (hoặc nước rửa tay) để sát khuẩn\\n\\n- Bước 2: Bật nước và rửa tay dưới dòng nước sạch.',\n",
       " '- Bước 3: Tắt vòi nước và kiểm tra kĩ, tránh trường hợp nước chảy nhỏ giọt.',\n",
       " '- Lưu ý:\\n\\n+ Bạn không được vứt rác, tóc, giấy vào bồn rửa tay, tránh trường hợp tắc ống.',\n",
       " '+ Không được tác động mạnh vào bồn.',\n",
       " '3.',\n",
       " 'Hướng dẫn sử dụng máy sấy tay:\\n\\n- Bước 1: Đưa tay vào dưới máy sấy, úp tay, duỗi thẳng các ngón tay.',\n",
       " 'Khi có gió (khí) thổi xuống, bạn đưa tay của mình ra vào từ từ, nhiều lần.',\n",
       " '- Bước 2: Ngửa lòng bàn tay lên và giữa nguyên thao tác như bước 1.',\n",
       " '- Bước 3: Xoa hai tay với nhau để tay khô hẳn.',\n",
       " 'Lưu ý: máy sấy sẽ tự động tắt khi hoạt động liên tục trong 60 giây.',\n",
       " 'Nếu muốn tiếp tục sử dụng, bạn hãy đưa tay ra khỏi thiết bị và làm lại lần lượt 3 bước trên.']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "da498c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stopwords(filepath):\n",
    "    lines = []\n",
    "\n",
    "    # Open the file for reading\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        # Read each line in the file\n",
    "        for line in file:\n",
    "            # Append each line to the list, removing leading and trailing whitespace\n",
    "            lines.append(line.strip())\n",
    "            \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9e9d5cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_stopwords = read_stopwords(\"./vietnamese-stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "cardiac-andrew",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-20T07:25:59.322566Z",
     "iopub.status.busy": "2021-06-20T07:25:59.321864Z",
     "iopub.status.idle": "2021-06-20T07:25:59.325157Z",
     "shell.execute_reply": "2021-06-20T07:25:59.324631Z",
     "shell.execute_reply.started": "2021-06-20T07:20:15.247977Z"
    },
    "id": "PbsfOh6URTll",
    "papermill": {
     "duration": 0.024813,
     "end_time": "2021-06-20T07:25:59.325300",
     "exception": false,
     "start_time": "2021-06-20T07:25:59.300487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#class for preprocessing and creating word embedding\n",
    "\n",
    "class Preprocessing:\n",
    "    #constructor\n",
    "    def __init__(self,txt):\n",
    "        # Tokenization\n",
    "        nltk.download('punkt')  #punkt is nltk tokenizer \n",
    "        # breaking text to sentences\n",
    "        tokens = underthesea.sent_tokenize(txt) \n",
    "        self.tokens = tokens\n",
    "        self.tfidfvectoriser=TfidfVectorizer()\n",
    "        self.word2vect_model = self.initWord2Vect()\n",
    "\n",
    "    # Data Cleaning\n",
    "    # remove extra spaces\n",
    "    # convert sentences to lower case \n",
    "    # remove stopword\n",
    "    def clean_sentence(self, sentence, stopwords=False):\n",
    "        sentence = sentence.lower().strip()\n",
    "        sentence = re.sub(r'[^a-zA-Z0-9\\sÀÁẢẠÃĂẰẮẲẶẴÂẦẤẨẬẪÈÉẺẸẼÊỀẾỂỆỄÌÍỈỊĨÒÓỎỌÕÔỒỐỔỘỖƠỜỚỞỢỠÙÚỦỤŨƯỪỨỬỰỮỲÝỶỴỸàáảạãăằắẳặẵâầấẩậẫèéẻẹẽêềếểệễìíỉịĩòóỏọõôồốổộỗơờớởợỡùúủụũưừứửựữỳýỷỵỹđ]+', '', sentence)\n",
    "        if stopwords:\n",
    "            sentence = remove_stopwords(sentence)\n",
    "        return sentence\n",
    "\n",
    "    # store cleaned sentences to cleaned_sentences\n",
    "    def get_cleaned_sentences(self,tokens, stopwords=False):\n",
    "        cleaned_sentences = []\n",
    "        for line in tokens:\n",
    "            cleaned = self.clean_sentence(line, stopwords)\n",
    "            cleaned_sentences.append(cleaned)\n",
    "        return cleaned_sentences\n",
    "\n",
    "    #do all the cleaning\n",
    "    def cleanall(self):\n",
    "        cleaned_sentences = self.get_cleaned_sentences(self.tokens, stopwords=True)\n",
    "        cleaned_sentences_with_stopwords = self.get_cleaned_sentences(self.tokens, stopwords=False)\n",
    "        return [cleaned_sentences,cleaned_sentences_with_stopwords]\n",
    "\n",
    "    def initWord2Vect(self):\n",
    "        # Get it from https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.vi.vec\n",
    "        vectors_gzip = \"./weights/wiki.vi.model.bin.gz\"\n",
    "        vectors_loc=\"./weights/wiki.vi.model.bin\"\n",
    "        if not os.path.exists(vectors_loc):\n",
    "            with gzip.open(vectors_gzip, 'rb') as f_in:\n",
    "                with open(vectors_loc, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "        model = KeyedVectors.load_word2vec_format(vectors_loc, binary=True)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    # TF-IDF Vectorizer\n",
    "    def TFIDF(self,cleaned_sentences):\n",
    "        self.tfidfvectoriser.fit(cleaned_sentences)\n",
    "        tfidf_vectors=self.tfidfvectoriser.transform(cleaned_sentences)\n",
    "        return tfidf_vectors\n",
    "\n",
    "    #tfidf for question\n",
    "    def TFIDF_Q(self,question_to_be_cleaned):\n",
    "        tfidf_vectors=self.tfidfvectoriser.transform([question_to_be_cleaned])\n",
    "        return tfidf_vectors\n",
    "\n",
    "    # main call function\n",
    "    def doall(self):\n",
    "        cleaned_sentences, cleaned_sentences_with_stopwords = self.cleanall()\n",
    "        tfidf = self.TFIDF(cleaned_sentences)\n",
    "        word2vect = self.Word2Vect(cleaned_sentences)\n",
    "        return [cleaned_sentences,cleaned_sentences_with_stopwords,tfidf, word2vect]\n",
    "    \n",
    "    # Get word2vect\n",
    "    def Word2Vect(self, cleaned_sentences):\n",
    "        result = []\n",
    "        for sentence in cleaned_sentences:\n",
    "            sentence = [self.getWord2Vect(token) for token in list(gensim.utils.tokenize(sentence))]\n",
    "            result.append(np.mean(np.array(sentence), axis=0))\n",
    "        return result\n",
    "    \n",
    "    def getWord2Vect(self, word):\n",
    "        try:\n",
    "            vect = self.word2vect_model[word]\n",
    "        except:\n",
    "            vect = np.random.uniform(-0.25, 0.25, 400)\n",
    "        return vect\n",
    "\n",
    "    def sentenceWord2Vect(self, sentence):\n",
    "        word2vect_sen = [self.getWord2Vect(token) for token in list(gensim.utils.tokenize(sentence))]\n",
    "        return np.mean(np.array(word2vect_sen), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "dried-fraction",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-20T07:25:59.352347Z",
     "iopub.status.busy": "2021-06-20T07:25:59.351271Z",
     "iopub.status.idle": "2021-06-20T07:25:59.353781Z",
     "shell.execute_reply": "2021-06-20T07:25:59.354254Z",
     "shell.execute_reply.started": "2021-06-20T07:20:17.933044Z"
    },
    "id": "3Lk2YxIPRaC1",
    "papermill": {
     "duration": 0.020313,
     "end_time": "2021-06-20T07:25:59.354446",
     "exception": false,
     "start_time": "2021-06-20T07:25:59.334133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#class for answering the question.\n",
    "class AnswerMe:\n",
    "    #cosine similarity\n",
    "    def __init__(self, type_embedding=\"word2vect\"):\n",
    "        self.type_embedding = type_embedding\n",
    "    \n",
    "    def Cosine(self, question_vector, sentence_vector):\n",
    "        dot_product = np.dot(question_vector, sentence_vector.T)\n",
    "        denominator = (np.linalg.norm(question_vector) * np.linalg.norm(sentence_vector))\n",
    "        return dot_product/denominator\n",
    "    \n",
    "    #Euclidean distance\n",
    "    def Euclidean(self, question_vector, sentence_vector):\n",
    "        vec1 = question_vector.copy()\n",
    "        vec2 = sentence_vector.copy()\n",
    "        if self.type_embedding != \"word2vect\":\n",
    "            if len(vec1)<len(vec2): vec1,vec2 = vec2,vec1\n",
    "            vec2 = np.resize(vec2,(vec1.shape[0],vec1.shape[1]))\n",
    "        return np.linalg.norm(vec1-vec2)\n",
    "\n",
    "    # main call function\n",
    "    def answer(self, question_vector, sentence_vector, method):\n",
    "        if method==1: return self.Euclidean(question_vector,sentence_vector)\n",
    "        else: return self.Cosine(question_vector,sentence_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "incoming-globe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-20T07:25:59.376514Z",
     "iopub.status.busy": "2021-06-20T07:25:59.375528Z",
     "iopub.status.idle": "2021-06-20T07:25:59.383100Z",
     "shell.execute_reply": "2021-06-20T07:25:59.382443Z",
     "shell.execute_reply.started": "2021-06-20T07:22:02.908309Z"
    },
    "id": "xjbJiKySRdbs",
    "papermill": {
     "duration": 0.019575,
     "end_time": "2021-06-20T07:25:59.383240",
     "exception": false,
     "start_time": "2021-06-20T07:25:59.363665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RetrieveAnswer(question_embedding, tfidf_vectors,method=1, type_embedding=None):\n",
    "    similarity_heap = []\n",
    "    if method==1: max_similarity = float('inf')\n",
    "    else: max_similarity = -1\n",
    "    index_similarity = -1\n",
    "\n",
    "    for index, embedding in enumerate(tfidf_vectors):  \n",
    "        find_similarity = AnswerMe(type_embedding)\n",
    "        print(type(embedding))\n",
    "        if not isinstance(question_embedding, np.ndarray):\n",
    "            question_embedding = (question_embedding).toarray()\n",
    "        if not isinstance(embedding, np.ndarray):\n",
    "            embedding = (embedding).toarray()\n",
    "        similarity = find_similarity.answer(question_embedding,embedding , method).mean()\n",
    "        if method==1:\n",
    "            heapq.heappush(similarity_heap,(similarity,index))\n",
    "        else:\n",
    "            heapq.heappush(similarity_heap,(-similarity,index))\n",
    "            \n",
    "    return similarity_heap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "acceptable-oxford",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-20T07:25:59.405463Z",
     "iopub.status.busy": "2021-06-20T07:25:59.404787Z",
     "iopub.status.idle": "2021-06-20T07:25:59.407734Z",
     "shell.execute_reply": "2021-06-20T07:25:59.407223Z",
     "shell.execute_reply.started": "2021-06-20T07:22:17.074001Z"
    },
    "id": "KwSarMVVRff8",
    "papermill": {
     "duration": 0.015849,
     "end_time": "2021-06-20T07:25:59.407896",
     "exception": false,
     "start_time": "2021-06-20T07:25:59.392047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put Your question here\n",
    "user_question = \"Cho tôi hướng dẫn sử dụng máy sấy tay\"\n",
    "#define method\n",
    "method = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "moved-alias",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-20T07:25:59.439696Z",
     "iopub.status.busy": "2021-06-20T07:25:59.438987Z",
     "iopub.status.idle": "2021-06-20T07:25:59.695975Z",
     "shell.execute_reply": "2021-06-20T07:25:59.695306Z",
     "shell.execute_reply.started": "2021-06-20T07:22:19.932569Z"
    },
    "id": "JJR4MfGRRobc",
    "papermill": {
     "duration": 0.279254,
     "end_time": "2021-06-20T07:25:59.696126",
     "exception": false,
     "start_time": "2021-06-20T07:25:59.416872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tuannm/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuannm/anaconda3/envs/common/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/tuannm/anaconda3/envs/common/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "preprocess = Preprocessing(txt)\n",
    "cleaned_sentences,cleaned_sentences_with_stopwords,tfidf_vectors, word2vect_dict = preprocess.doall()\n",
    "\n",
    "question = preprocess.clean_sentence(user_question, stopwords=True)\n",
    "question_embedding = preprocess.TFIDF_Q(question)\n",
    "question_word2vect = preprocess.sentenceWord2Vect(question)\n",
    "\n",
    "similarity_heap = RetrieveAnswer(question_embedding , tfidf_vectors ,method, \"tfidf\")\n",
    "\n",
    "# Using word2vect\n",
    "# similarity_heap = RetrieveAnswer(question_word2vect, word2vect_dict ,method )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "controlling-engagement",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-20T07:25:59.721770Z",
     "iopub.status.busy": "2021-06-20T07:25:59.720648Z",
     "iopub.status.idle": "2021-06-20T07:25:59.724662Z",
     "shell.execute_reply": "2021-06-20T07:25:59.724186Z",
     "shell.execute_reply.started": "2021-06-20T07:22:21.061904Z"
    },
    "id": "r-TZFjAERflE",
    "papermill": {
     "duration": 0.019155,
     "end_time": "2021-06-20T07:25:59.724819",
     "exception": false,
     "start_time": "2021-06-20T07:25:59.705664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Cho tôi hướng dẫn sử dụng máy sấy tay\n",
      "hướng dẫn sử dụng máy sấy tay\n",
      "\n",
      " bước 1 đưa tay vào dưới máy sấy úp tay duỗi thẳng các ngón tay\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(\"Question: \", user_question)\n",
    "\n",
    "# number of relevant solutions you want here it will print 2\n",
    "number_of_sentences_to_print = 2\n",
    "\n",
    "while number_of_sentences_to_print>0 and len(similarity_heap)>0:\n",
    "    x = similarity_heap.pop(0)\n",
    "    print(cleaned_sentences_with_stopwords[x[1]])\n",
    "    number_of_sentences_to_print-=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.772855,
   "end_time": "2021-06-20T07:26:00.570891",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-20T07:25:48.798036",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
